# Optimization Solver Benchmark Test
Author: Connor Chai
Date: August 22 2025

# Scope
Benchmark OASIS.AI against Dakota and Python open-source optimizers

## A Brief Note On How Dakota Works
Dakota is a framework that is used for engineering analysis and can perform a number of tasks, such as uncertainty quantification and optimization. When using it to run optimization problems, there are a few things the user should be aware of, including the files types and the 6 types of blocks in a Dakota `.in` file. Version 22.0 was used for this benchmark test.

Dakota code is written in `.in` files, and the format of the output logs are in `.out` files (although empty `LHS...out` files are also generated). For optimization cases, a `.dat` file is generated by a specification in the `environment` block, which contains the inputs and outputs (objective function value(s), constraint(s) values) on the function defined by the driver specified in the `interface` block. In `.in` files for optimization, (asides from those where SBO is implemented), the `method` block specifies the optimization algorithm, `model` block specifies the way the algorithm is implemented, `variables` block specifies the variable types (continuous/discrete) and bound constraints, and the `responses` block specifies the types of output that Dakota expects from the driver code (defines objective function, along with constraints, if any) in the `interface` block. Dakota also generates several other empty files in the process, all of which can be ignored. In short, to run a Dakota optimization file with `dakota -i [solver_name].in -o [solver_name].out` , you need a `.in` file and a `.py` driver file.

## Single-Objective Optimization (SOO) Benchmark (Unconstrained)(Python/Dakota/Oasis)
The code for running SOO problems is divided into a handful of scripts, one for Dakota, one for OASIS's raw data, and the rest for Python. Each solver across the three frameworks are run 10 times for each problem, using seeds which are defined in the code. The file structure is consistent across all problems, which each have their own folder: a single folder for each of the solvers that was run on the problem, each containing the raw `.dat` files for each of the 10 instances and three `.tsv` files with data of runtimes, average running minima across instances for each function evaluation # (for making convergence plots), and the best minima per instance (for making bar plots).

### Dakota (`dakota_batch_optimizer`)
This script automates the process of running Dakota `.in` files, and adds the running minima data to the raw data files. In order to run the Dakota automation script, `dakota_batch_optimizer`, a series of driver and templates for `.in` files were set up in the `drivers_and_templates` folder and disseminated to each solver subfolders using the `setup()` function toggled with the `setup` flag. After the `.in` template files were manually modified, the solvers can be run on the specified problem by adjusting the parameters in the call to `main()`. A quick note: the time data is only written after all the specified instances are run, and this is universal across all the batch optimizer scripts.

### Python (`python_batch_optimizer.py`, `python_solvers.py`, `python_definitions.py`, `python_functions.py`)
Python 3.11 was used for this benchmark test. In order to run the Python automation script, it is important to understand the file layout. The list of problems (function name + dimension) in `python_batch_optimizer.py` are interpreted using the dictionary definitions set out in `python_definitions`, where the function name, dimensionality, bounds, and budget are defined. This helper script refers to `python_problems_and_constraints` where the raw objective functions are defined, regardless of dimensions. The `python_solver` defines the solver specifications. Similarly to the Dakota batch script, the only parameters that need to be modified are in the call to main(), with `func_names`.

Several helper functions are included in `python_solvers.py` to help the Nevergrad and OMADS converge, as the initial guesses for several solvers on problems like Michalewicz60d yielded values as high as 10^70. The logic for the solvers were not modified as the raw objective function values were still logged and plotted while a transformed objective function value was passed to the solver itself. By doing this the solvers could more effectively explore the design space for the next objective function evaluation. Solver errors occured without this extra function, such as a LossTooLargeWarning in Nevergrad when solving for Michalewicz60d.

### Oasis (`oasis_processor.py`)
This simple script is called when running `grapher.py` and is used to post-processes OASIS.AI data from the `oasis_raw_results` folder to create the three data files used by `grapher.py` (summary of best minima per instance, average running minima per function evaluation, and runtime per instance). The results using OASIS.AI were genereated independent of the author of this report, and the raw data can be found in the respective MOO and SOO folders under `oasis_raw_results`

## GRAPHER (`grapher.py`)
For all problems listed in `problem_folders`, this script traverses all the solvers listed in `solver_names` to create a boxplot, convergence chart, and table outlining the runtime for each instance for each of the solvers, using files from each solver subfolder. 

### Other comments on SOO unconstrained solver results:
- The initial test results on Rosenbrock 10d with genie_direct, genie_opt, coliny_direct, Ax-BO, and SMT-EGO are found under `rosenbrock10` -> `initial_test_results`
- When running Nevergrad, RunTimeWarning messages from Scipy libraries persist. But, the Nevergrad values are reasonable, so these messages were ignored. 
- Looking at the results, Scipy DIRECT was the only solver that went significantly over the budget (>~50) for any given function. The reason is unknown.
- The `.dat` file from the third instance of genie_direct on schubert10d is empty. This is likely due to an internal kink in Dakota and the results were averaged without data from that instsance
- The convergence plots strictly shows the running minima up to 1000 budget evaluations. In the cases where solvers go over the alloted budget, there is not signficant improvement after 1000 budget evaluations. If desired, the user may modify `graph.py` and change the number of points that are plotted.
- There was an attempt to implement a helper function like solve() in the OMADS wrapper for all solvers, but this turned out to be a unnecessary and a waste of time. Not only did it barely improve the results for Scipy solvers, but it negatively impacted the results for the higher dimension problems by several magnitudes. For example, in Schubert60, when passing this helper function, instead of the raw objective function to the Nevergrad solver, the objective function value decreased 5-fold, from about 10^-25 to 10^-20. The results for all solvers for all problems for this attempt at a universal transformation function can be found in the repository under `results_modified_solver.py_file`
- - After the optimization process is completed when running `dakota_batch_optimizer.py` for Rosenbrock50d and Michalewicz60d, there is this error `...\dakota_batch_optimizer.py:222: RuntimeWarning: Mean of empty slice avg = np.nanmean(per_instance_vals)`, which has been ignored for now as the data seems to still convergence to an expected minimum 

## Single-Objective Optimization (SOO) Benchmark (Constrained)(Python/Dakota/Oasis)
### Python (`python_batch_optimizer.py', `python_solvers.py`)
Python solvers, including Ax-BO, did not return any viable solutions, but the code remains in the `gseven10_constrained` subfolder. The code is a slightly older version of the one used in SOO unconstrained, but the logic is the same and the two files defining problems, constraints, and functinos were combined into `python_batch_optimizer.py` to improve readability.

### Dakota (`gseven_dakota_batch.py`)
The code for Dakota constrained SO was set up in the same way as for the batch automation script for SOO unconstrained's `dakota_batch_optimizer.py`. The only difference is that there are now several helper functions to check for feasibility of output values and a modified convergence plot that takes into account feasibility of objective function values.

`run_optimization()` initializes the three files needed for keeping track of time and for plotting the bar graph and convergence chart, which is toggled with setup = True. The function allows for the modification and execution of .in file for each instance of each solvers (helper method is modify_and_run). `append_flag()` iterates through each .dat file of each solver's instances for each NFE and adds flags. `congregate_convergence()` displays a table with best running minima for each NFE, as well as the best running minima. `count_feasible()` creates a new file for each method, counting the number of feasible evaluations at each NFE across all instances. Both are commented out in the script, as they were only helpful for intermediate data analysis. `calculate_coliny_single()`, `calculate_sbo()`, and `calculate_oasis()` creates a new file for each solver for a set of feasible solutions. `plot_convergence_from_files()`, creates a plot and graphs the convergence plot for each data. 

## Bi-Objective Optimization (MOO) Benchmark (Python/Dakota/Oasis)
The structure of the code for running the MOO problems is very similar to that of SOO.

### DAKOTA (`dakota_batch_optimization.py`)
The script that modifies and runs the Dakota .in files for each instance of each solver in order to raw .dat files which are then used to generate .tsv files with f1 f2 pairs of non-dominated solutions. The accompanying files for running the Dakota code are found in  `drivers_and_templates`, similarly to the SOO folder of the same name. The output for each solver includes the raw data from each instance, a .tsv file with the non-dominated solutions, and a .tsv file with instance runtimes. When the setup flag is set to true, the drivers and Dakota files for each problem are copied into the respective problem folders. Setting the setup flag to false allows for the script to run the optimization process as normal.

### PYTHON (`python_batch_optimization.py`, `python_definitions`, `python_problems_and_constraints.py`, `python_solvers`, and `python_OMADS_MOO`)
A series of scripts that creates the same output as the Dakota script for each of the solvers defined in Python. `graphy.py` iterates through the data from each solver for each problem to create Pareto Frontier graphs instead of boxplots and convergence charts. `python_batch_optimizer.py' runs the Python solvers on the suite of problems, with the help of `python_solvers.py`, `python_definitions`, and `python_problems_and_constraints` to define the raw functions (with constraints when necessary), presets for solving functions (dimensionality, budget, bounds), and solvers (with the help of wrapper classes). Additionally, an extra file was created for implementing OMADS on this suite of problems, `python_OMADS_MOO.py`, which runs and post-processes the data from OMADS independen of other scripts. OMADS required a different format of definitions for the functions and problems. ` 

A Pareto Frontier is the set of non-dominated solutions which can be used to visually compare bi-objective solvers. These are created by iterating through the {solver}_pareto.tsv files for each solver. If it is a constrained problem, only non-dominating solutions that satisfy the constraints are recorded and plotted as the Pareto Frontier. The Pareto Froniter filtering is done inside the main batch optimization script for the three Pymoo implementations and OASIS.AI. The comparison of hypervolume calculations for each solver was alos implemented. The hypervolume of a solver for a given problem gives a relative value to the rest of the solvers for how close the set of non-dominated solutions are to the origin. A higher hypervolume indicates higher performance.

As stated in the report, the OMADS parameters were copied from the OMADS repository, with one exceptionâ€” setting constraints_type = [] instead of ["PB"]. The Pareto Frontier for OMADS on each of the problems was noticeably improved. Additionally, when running the OMADS script on OSY for several of the instances, the following RuntimeWarning messages appear: invalid value encountered in scalar divide, invalid value encountered in sqrt, and divide by zero encountered in divide. This is likely due to values getting too large inside of the OMADS during the constraint steps, and the effects of this on the accuracy of the solver are unknown. 

### Installing dependancies
Necessary dependancies for running both single-objective and bi-objective problems can be installed with the following command: `pip install numpy matplotlib scipy nevergrad pymoo torch botorch ax-platform OMADS` 
