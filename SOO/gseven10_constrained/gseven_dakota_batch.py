# gseven_dakota_batch.py, runs the 5 Dakota single objective optimization solvers that can handle nonlinear constraints on the G7 problem defined by func name + dimention). Setup will be necessary for setting up .in and driver files.

import pandas as pd
import numpy as np
import os, re, subprocess, time, glob
import matplotlib.pyplot as plt
import shutil

"""
Helper method, modify the Dakota .in file and run Dakota.
"""
def modify_and_run(dakota_input, dakota_output, line_indices_to_edit, index_of_number, method):
    # identify dakota .in file
    method_folder = f"{method}"
    dakota_input_path = os.path.join(method_folder, dakota_input)
    print(dakota_input_path)

    # read the file into memory
    with open(dakota_input_path, 'r') as f:
        lines = f.readlines()

    # modify the target lines
    for line_index in line_indices_to_edit:
        if line_index < len(lines):
            line = lines[line_index].strip()
            parts = line.split()

            if not parts:
                print("The line you want to change is empty")
                continue

            last_token = parts[-1]

            # modify the specified output file (.dat) name inside of Dakota (.in) file
            if last_token.startswith("'") and last_token.endswith(".dat'"):
                # replace trailing number before .dat using regex
                new_filename = re.sub(
                    r'(\D+)(\d+)(\.dat)',
                    lambda m: f"{m.group(1)}{index_of_number}{m.group(3)}",
                    last_token.strip("'")
                )
                parts[-1] = f"'{new_filename}'"
                lines[line_index] = '  '.join(parts) + '\n'

            # modify the seed number (uses the same specification as solver run number)
            else:
                try:
                    float(last_token)  # Is it a number?
                    parts[-1] = str(index_of_number)
                    lines[line_index] = '  '.join(parts) + '\n'
                except ValueError:
                    print(f"Skipping line {line_index + 1}: unknown format")
        else:
            print(f"Line {line_index + 1} does not exist in the file.")
    
    # write the updated lines back to the file
    with open(dakota_input_path, 'w') as f:
        f.writelines(lines)
    if not os.path.exists(dakota_input_path):
        raise FileNotFoundError(f"{dakota_input_path} not found — check file creation logic.")

    # run in Dakota .in file in the appropriate method folder
    subprocess.run(['dakota', '-i', dakota_input, '-o', dakota_output], check=True, cwd=method_folder)           #attend to this

    # remove unnecessary files generated by sbo
    if method == "sbo":
        # Remove all files starting with 'finaldatatruth' in method_folder
        for file_path in glob.glob(os.path.join(method_folder, 'finaldatatruth*')):
            try:
                os.remove(file_path)
            except FileNotFoundError:
                pass  # Safe to ignore if a file was deleted concurrently or doesn't exist


"""
Run each method `instances` times, modifying and executing its Dakota input.
Assumes append_running_min(path, ndim) exists and produces results_{method}{j}.dat.
"""
def run_optimization(ndim, method_types):
    # iterate through the 5 methods
    for method in method_types:
        method_times = []                                                     # create a 1d list for keeping track of runtimes for THIS method only
        dakota_input_file = f"{func}{ndim}d_{method}.in"                      # reference the dakota file
        if method == "sbo":
            lines_to_change = [4, 23]
        elif method == "NCSU":
            lines_to_change = [4]              # define the lines in the dakota file to be modified                       
        else:
            lines_to_change = [4, 8]
        # run the method 10 times (from index 1 to 10)
        for j in range(1, instances+1):
            start = time.time()
            dakota_output_file = f"{func}{ndim}d_{method}{j}.out"
            modify_and_run(dakota_input_file, dakota_output_file, lines_to_change, index_of_number = j, method=method)      # change the seed and run to create .dat file
            end = time.time()
            elapsed = end-start
            method_times.append(f"{elapsed:.2f}")
            append_running_min(os.path.join(f"{method}", f"results_{method}{j}.dat"), ndim)             # add running minimum to the .dat file


"""
Iterate through every row of every .dat file and check if constraints are satisfied
"""
def append_flag(ndim, instances, tolerance, method_types):
    original_num_cols = ndim + 11
    
    for method in method_types:
        for i in range(1, instances+1):
            source_file = os.path.join(f"{method}", f"results_{method}{i}.dat")
            
            with open(source_file, 'r') as f:
                lines = f.readlines()

            header_lines = lines[:2]
            data_lines = lines[2:]
            updated_data_lines = []
        

            for line in data_lines:
                valuese = line.strip().split() #1d array with values
                values = valuese[:original_num_cols]
                
                constraint_values = list(map(float, values[ndim + 3 : ndim + 11]))  # 13th–21st cols has constraint values

                all_satisfied = all(c < tolerance for c in constraint_values)
                if all_satisfied:
                    updated_line = "\t".join(values) + f"\t{all_satisfied}" + f"\t{valuese[ndim+2]}"
                else: 
                    updated_line = "\t".join(values) + f"\t{all_satisfied}" + f"\t{np.nan}"
                updated_data_lines.append(updated_line)

            # write the updated file with new column
            with open(source_file, 'w') as f:
                original_header_cols = header_lines[0].strip().split()[:original_num_cols]
                cleaned_header = "\t".join(original_header_cols) + "\thas_constraints" + "\tfeasible_running_minima"  # adds headers for constraint flag 
                f.write(cleaned_header + "\n")
                f.write(header_lines[1])
                for line in updated_data_lines:
                    f.write(line + '\n')

# appends the running minimum to the .dat file 
def append_running_min(ndim, method_types, instances):
    for method in method_types:
        for j in range(1, instances+1):
            source_file = os.path.join(f"{method}", f"results_{method}{j}.dat") 

            # read data with header
            with open(source_file, 'r') as f:
                lines = f.readlines()

            header_lines = lines[:2]
            data_lines = lines[2:]

            data = np.genfromtxt(source_file, skip_header=2)                        # load data, skipping header row and the initial guess
            obj_values = data[:, ndim + 2]                                          # create array with objective function values, obj values are found in ndim+2nd column
            running_min = np.minimum.accumulate(obj_values)                         # determine the running minimums in the column array
            
            # write the updated file with new column
            with open(source_file, 'w') as f:
                # write both original header lines
                f.write(header_lines[0].strip() + '\tbest_so_far\n')
                f.write(header_lines[1].strip() + '\n')
                
                # rewrite data rows, with the desired column appended
                for i in range(len(data_lines)):
                    original_line = data_lines[i].strip()
                    f.write(f"{original_line}\t{running_min[i]}\n")

"""
Helper method, return the number of rows from the specified file
"""
def count_rows(file_path):
    try:
        with open(file_path, 'r') as f:
            num_rows = sum(1 for _ in f)
        return num_rows
    except FileNotFoundError:
        print(f"File not found: {file_path}")
        return 0

"""
Helper method, return the number of rows for each ..result_1, ..result_2 file for a given method
"""
def get_max_row_count(base_path, filename_pattern, file_suffix_range):
    max_rows = 0

    for i in file_suffix_range:
        file_name = filename_pattern.format(i)
        file_path = os.path.join(base_path, file_name)
        
        row_count = count_rows(file_path)
        if row_count > max_rows:
            max_rows = row_count

    return max_rows

"""
Create a .dat for each method, with the running best minima for each of the 10 runs. The average running minima is appended in the 11th column
"""
def congregate_convergence(method_types, instances, target_col_index):
    start_line = 2
    file_suffix_range = range(1, instances + 1)

    for method in method_types:
        method_folder = method
        convergence_output_file = os.path.join(method_folder, f"{method}_convergence.dat")

        max_rows = get_max_row_count(method_folder, f"results_{method}{{}}.dat", file_suffix_range)
        line_indices = range(start_line, max_rows)  # analogous to previous offset logic

        # prepare header: Run 1, Run 2, ..., Best Average Running Min
        header_parts = [f"Run {i}" for i in range(1, len(file_suffix_range) + 1)]
        header_parts.append("Best Average Running Min")

        try:
            with open(convergence_output_file, "w") as out_f:
                out_f.write("\t".join(header_parts) + "\n")
                prev_avg = np.inf

                for line_num in line_indices:
                    row_values = []

                    for i in file_suffix_range:
                        filename = os.path.join(method_folder, f"results_{method}{i}.dat")
                        if not os.path.exists(filename):
                            print(f"Instance file not found: {filename}")
                            row_values.append(np.nan)
                            continue

                        with open(filename, "r") as f:
                            lines = f.readlines()

                        if line_num >= len(lines):
                            row_values.append(np.nan)
                            continue

                        tokens = lines[line_num].strip().split()
                        try:
                            val = float(tokens[target_col_index])
                            row_values.append(val)
                        except (IndexError, ValueError):
                            print(f"Skipping invalid value in file {filename} at line {line_num+1}")
                            row_values.append(np.nan)

                    row_avg = np.nanmean(row_values)
                    best_to_write = row_avg if row_avg < prev_avg else prev_avg

                    # write raw values: use 'nan' for NaNs, otherwise default str()
                    serialized = [
                        "nan" if np.isnan(v) else str(v) for v in row_values
                    ]
                    best_str = "nan" if (np.isnan(best_to_write) or best_to_write == np.inf) else str(best_to_write)
                    out_f.write("\t".join(serialized) + "\t" + best_str + "\n")

                    if row_avg < prev_avg:
                        prev_avg = row_avg

            print(f"Created convergence summary: {convergence_output_file}")
        except Exception as e:
            print(f"Error writing convergence file for method {method}: {e}")

"""
Record the number of feasible solutions for each NFE across all 10 instances of each solver and append to each {method}_convergence.dat file
"""
def count_feasible(instances, method_types):
    for method in method_types:
            convergence_file = os.path.join(method, f"{method}_convergence.dat")     
            try:
                with open(convergence_file, 'r') as f:
                    lines = f.readlines()
            except FileNotFoundError:
                print(f"File not found: {convergence_file}")
                continue

            header_lines = lines[:1]
            data_lines = lines[1:]
            num_lines = len(data_lines)
            feasible_counts = []

            for j in range(num_lines):
                count_feasible = 0
                for i in range(1, instances + 1):
                    instance_file = f"{method}/results_{method}{i}.dat" 
                    try:
                        with open(instance_file, 'r') as f_inst:
                            inst_lines = f_inst.readlines()[2:]
                            if j >= len(inst_lines):
                                continue 
                            values = inst_lines[j].strip().split()
                            if len(values) > 21 and values[21].lower() == "true":  # MODIFY BASED ON NUMBER OF CONSTRAINTS AND OBJECTIVES
                                count_feasible += 1
                    except FileNotFoundError:
                        print(f"Instance file not found: {instance_file}")
                        continue

                feasible_counts.append(count_feasible)
            
            # now append counts to the convergence file
            output_lines = []
            output_lines.append(header_lines[0].strip() + "\tNumFeasible\n")  # add column header
            for line, count in zip(data_lines, feasible_counts):
                output_lines.append(line.strip() + f"\t{count}\n")

            # WRITE TO NEW FILE FOR NOW
            output_file = f"{method}/{method}_convergence_with_feasible.dat"
            with open(output_file, 'w') as f_out:
                f_out.writelines(output_lines)

            print(f"Appended feasible count column to {output_file}")
            
"""
Combine running minima data from all methods, from each individual {method}_convergence.dat file
"""
def master_congregate(method_types, output_file):
    all_data = []
    max_length = 0

    # iterates through the convergence .dat file for each method and appends the average 
    for method in method_types:
        file_path = os.path.join(f"{method}", f"{method}_convergence.dat")
        data = np.genfromtxt(file_path, skip_header=1)
        values = data[:, 10]  # 11th column of convergence file where the average is stored
        all_data.append(values)
        max_length = max(max_length, len(values))

    # pad all columns to the same length with np.nan, as some methods may have more runs than others
    padded_data = []
    for col in all_data:
        padded = np.full(max_length, np.nan)
        padded[:len(col)] = col
        padded_data.append(padded)
    combined = np.column_stack(padded_data)

    # write to output file
    with open(output_file, 'w') as f:
        # header
        f.write('\t'.join([f"{name:>10}" for name in method_types]) + '\n')
        # data rows
        for row in combined:
            f.write('\t'.join([f"{val:10.2e}" if not np.isnan(val) else "" * 12 for val in row]) + '\n')


"""

"""
def calculate_coliny_single():
    # Configuration
    num_nfe = 500
    objective_col = 12  # Column 13 (0-indexed)
    feasibility_col = 21  # Column V (0-indexed)
    filename = "COLINY/results_COLINY7.dat"

    average_objective_values = []

    if not os.path.exists(filename):
        print(f"File not found: {filename}")
        return

    # Read file
    df = pd.read_csv(filename, sep=r'\s+', skiprows=2, header=None)

    for nfe in range(2, num_nfe + 2):  # NFE from 1 to 500
        if len(df) < nfe:
            break
        row = df.iloc[nfe - 2]
        obj_value = float(row.iloc[objective_col])
        feasible = str(row.iloc[feasibility_col]).strip().lower() == "true"

        if feasible:
            avg_obj = obj_value
            feasible_found = True
        else:
            avg_obj = obj_value  # Still include value, just flagged
            feasible_found = False

        average_objective_values.append((nfe, avg_obj, feasible_found))

    # Compute best running minima
    running_min = []
    current_min = float('inf')
    feasible_flag_triggered = False

    for nfe, avg_obj, feasible_found in average_objective_values:
        if np.isnan(avg_obj):
            running_min.append((nfe, np.nan))
            continue
        if not feasible_flag_triggered:
            if feasible_found:
                current_min = avg_obj
                feasible_flag_triggered = True
            else:
                current_min = min(current_min, avg_obj)
        else:
            current_min = min(current_min, avg_obj)
        running_min.append((nfe, current_min))

    # Output results
    df_output = pd.DataFrame(running_min, columns=["NFE", "BestRunningMin"])
    output_file = "coliny_convergence_data.dat"
    df_output.to_csv(output_file, sep='\t', index=False)
    print(df_output.head())

def calculate_sbo():
    # Configuration
    num_instances = 10
    num_nfe = 500
    objective_col = 12  # Column 13 (0-indexed)
    feasibility_col = 21  # Column 22 (0-indexed)
    file_template = "sbo/results_sbo{}.dat"

    # Initialize storage
    average_objective_values = []

    # Process each NFE
    for nfe in range(2, num_nfe + 2):  # Adjusted for 1-indexed lines with header
        obj_values = []
        feasible_flags = []

        for i in range(1, num_instances + 1):
            filename = file_template.format(i)
            if not os.path.exists(filename):
                continue
            df = pd.read_csv(filename, sep=r'\s+', skiprows=2, header=None)
            if len(df) < nfe:
                continue
            row = df.iloc[nfe - 2] 
            obj_value = float(row[objective_col])
            feasible = str(row[feasibility_col]).strip().lower() == "true"
            # print(f"contents in the flagged row:{row[feasibility_col]} and the row index, to double check {nfe} for instance {i}")
            obj_values.append(obj_value)
            feasible_flags.append(feasible)
            
        # Separate feasible and infeasible values
        feasible_obj_values = [val for val, feas in zip(obj_values, feasible_flags) if feas]
        # print(f"feasible values for run NFE {nfe}: {feasible_obj_values}")
        infeasible_obj_values = [val for val, feas in zip(obj_values, feasible_flags) if not feas]

        # print(feasible_obj_values)
        # Compute average based on feasibility
        if feasible_obj_values:
            avg_obj = np.mean(feasible_obj_values)
            feasible_found = True
        else:
            avg_obj = np.mean(infeasible_obj_values) if infeasible_obj_values else np.nan
            feasible_found = False

        average_objective_values.append((nfe - 1, avg_obj, feasible_found))

    # Compute running minimums
    running_min = []
    current_min = float('inf')
    feasible_flag_triggered = False

    for nfe, avg_obj, feasible_found in average_objective_values:
        if np.isnan(avg_obj):
            running_min.append((nfe, np.nan))
            continue
        if not feasible_flag_triggered:
            if feasible_found:
                current_min = avg_obj
                feasible_flag_triggered = True
            else:
                current_min = min(current_min, avg_obj)
        else:
            current_min = min(current_min, avg_obj)
        running_min.append((nfe, current_min))

    
    # Convert to DataFrame and save
    df_output = pd.DataFrame(running_min, columns=["NFE", "BestRunningMin"])
    output_file = "sbo_convergence_data.dat"
    df_output.to_csv(output_file, sep='\t', index=False)
    print(df_output.head())

def calculate_oasis():
    # Configuration
    num_instances = 10
    num_nfe = 500
    objective_col = 18  # Column "S" (0-indexed)
    feasibility_col = 27  # Column "AB" (0-indexed)
    file_template = "OASIS_G7_RESULTS/Run no {} - G7 - result-table.csv"

    average_objective_values = []

    for nfe in range(2, num_nfe + 2):  # 1-based NFE indexing
        obj_values = []
        feasible_flags = []

        for i in range(1, num_instances + 1):
            filename = file_template.format(i)
            if not os.path.exists(filename):
                continue
            df = pd.read_csv(filename, header=0)
            if len(df) < nfe:
                continue
            row = df.iloc[nfe - 2]  # Adjust for 1 header line
            obj_value = float(row.iloc[objective_col])
            feasible = str(row.iloc[feasibility_col]).strip().lower() == "true"
            obj_values.append(obj_value)
            
            feasible_flags.append(feasible)

        # Separate feasible and infeasible values
        feasible_obj_values = [val for val, feas in zip(obj_values, feasible_flags) if feas]
        infeasible_obj_values = [val for val, feas in zip(obj_values, feasible_flags) if not feas]

        # Compute average
        if feasible_obj_values:
            avg_obj = np.mean(feasible_obj_values)
            feasible_found = True
        else:
            avg_obj = np.mean(infeasible_obj_values) if infeasible_obj_values else np.nan
            feasible_found = False

        average_objective_values.append((nfe - 1, avg_obj, feasible_found))

    # Running minimum logic
    running_min = []
    current_min = float('inf')
    feasible_flag_triggered = False

    for nfe, avg_obj, feasible_found in average_objective_values:
        if np.isnan(avg_obj):
            running_min.append((nfe, np.nan))
            continue
        if not feasible_flag_triggered:
            if feasible_found:
                current_min = avg_obj
                feasible_flag_triggered = True
            else:
                current_min = min(current_min, avg_obj)
        else:
            current_min = min(current_min, avg_obj)
        running_min.append((nfe, current_min))

    # Output
    df_output = pd.DataFrame(running_min, columns=["NFE", "BestRunningMin"])
    output_file = "oasis_convergence_data.dat"
    df_output.to_csv(output_file, sep='\t', index=False)
    print(df_output.head())

def plot_convergence_from_files():
    # Define file names, labels, and line styles
    file_info = {
        "coliny_convergence_data.dat": ("COLINY", "-"),
        "sbo_convergence_data.dat":    ("SBO",    "--"),
        "oasis_convergence_data.dat":  ("OASIS",  "-.")
    }

    plt.figure(figsize=(10, 6))

    for file_name, (label, linestyle) in file_info.items():
        try:
            # load and trim to first 498 rows
            df = pd.read_csv(file_name, sep='\t', header=0)
            df = df.iloc[:498]
        except FileNotFoundError:
            print(f"❌ File not found: {file_name}")
            continue
        except Exception as e:
            print(f"⚠️ Error reading {file_name}: {e}")
            continue

        # find the turning point index
        if label == "SBO":
            turning_idx = 100
        else:
            diffs = df["BestRunningMin"].diff()
            increases = diffs > 0
            turning_idx = int(increases.idxmax()) if increases.any() else len(df)

        # split into pre- and post-spike segments
        x = df["NFE"]
        y = df["BestRunningMin"]
        x_pre, y_pre   = x.iloc[:turning_idx], y.iloc[:turning_idx]
        x_post, y_post = x.iloc[turning_idx:], y.iloc[turning_idx:]

        # plot
        plt.plot(x_pre,  y_pre,  linestyle=linestyle, color="red",   label=f"{label} (infeasible)")
        plt.plot(x_post, y_post, linestyle=linestyle, color="green", label=f"{label} (feasible)")

    # Formatting
    plt.title("G7 Convergence Comparison: COLINY vs SBO vs OASIS")
    plt.xlabel("Function Evaluations (NFE)")
    plt.ylabel("Best Running Average Objective")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig("gseven_convergence_plot.png", dpi=300)
    plt.show()


# setup files and define methods to be run based on number of objectives (as constraints are defined within Dakota)
def main(func, ndim, instances, method_types, tolerance, setup=False):
    function_folder = f"{func}{ndim}d"                            # the folder where all the files for the problem will be stored

    #run_optimization(ndim, method_types)
    
    #append_flag(ndim, instances, tolerance, method_types)
    #append_running_min(ndim, method_types, instances)
    congregate_convergence(method_types, instances, target_col_index=(ndim + 3 + 8)) # add 8 because of nonlinear constraints, feasible running minima column
    # count_feasible(instances, method_types)
    calculate_coliny_single()
    calculate_sbo()
    calculate_oasis()
    plot_convergence_from_files()

def setup(func, ndim, method_types):
    # Define source files
    driver_file = f"{func}_driver.py"
    input_file = f"{func}{ndim}d.in"

    # Check that the required files exist in the current working directory
    if not os.path.isfile(driver_file):
        raise FileNotFoundError(f"Missing required file: {driver_file}")
    if not os.path.isfile(input_file):
        raise FileNotFoundError(f"Missing required file: {input_file}")

    for method in method_types:
        os.makedirs(method, exist_ok=True)

        # Copy files into the method folder
        shutil.copy(driver_file, method)
        new_input_filename = f"{func}{ndim}d_{method}.in"
        if method != "sbo":
            shutil.copy(input_file, os.path.join(method, new_input_filename))
        else:
            shutil.copy(input_file, os.path.join(method, f"{func}{ndim}d_sbo.in"))

if __name__ == "__main__":
    func = "gseven"
    ndim = 10
    setup_flag = False
    instances = 10
    method_types = ["COLINY", "ea", "ego", "sbo", "soga"]
    tolerance = 1e-6


 
    main(func, ndim, instances, method_types, tolerance, setup_flag)